--- transformers/models/bert/modeling_bert.py	2022-07-26 23:23:47.248104004 +0000
+++ /usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py	2023-01-07 08:46:04.772631406 +0000
@@ -223,12 +223,6 @@
         embeddings = self.dropout(embeddings)
         return embeddings
 
-class MM(nn.Module):
-    def __init__(self):
-        super().__init__()
-
-    def forward(self, a, b):
-        return torch.matmul(a, b)    
 
 class BertSelfAttention(nn.Module):
     def __init__(self, config):
@@ -247,9 +241,7 @@
         self.key = nn.Linear(config.hidden_size, self.all_head_size)
         self.value = nn.Linear(config.hidden_size, self.all_head_size)
 
-        self.softmax = nn.Softmax(dim=-1)
         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
-        self.mm = MM()
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
             self.max_position_embeddings = config.max_position_embeddings
@@ -334,7 +326,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = self.softmax(attention_scores)
+        attention_probs = nn.Softmax(dim=-1)(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -344,7 +336,7 @@
         if head_mask is not None:
             attention_probs = attention_probs * head_mask
 
-        context_layer = self.mm(attention_probs, value_layer)
+        context_layer = torch.matmul(attention_probs, value_layer)
 
         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
